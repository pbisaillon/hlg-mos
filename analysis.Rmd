---
title: "Hackathon"
author: "INS"
date: "25/01/2022"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)

# Load in libraries and retrieve data -------------------------------------
library(dplyr)
library(synthpop)
library(reshape2)
library(emdist)
library(ztable)
library(magrittr)
library(ggplot2)
library(plotly)
library(reticulate)
# To replicate, create a conda env and run
# conda install pandas seaborn scipy
use_python("./conda_env/bin/python")

# Source R script from GitHub
satgpa <-
  readr::read_csv("./data/satgpa.csv") %>%
  select(-sat_sum) 
  # removed this from the data set because it's dependent on sat_m and sat_v

synth_bisaillon <-
  readr::read_csv("./synthpop/synth_data.csv")

# Get upper triangle of the correlation matrix
## this will remove the top half of the values for the heat map for a cleaner look
get_upper_tri <- function(cormat) {
  cormat[lower.tri(cormat)] <- NA
  return(cormat)
}
```
## R synthpop

The synthetic data was produced using the function `syn()` and the summaries below were produced from using the function `summary()` from the `synthpop` library.

### Original data

```{r sum_og, echo=FALSE}
summary(satgpa)
```

### Synthetic data

```{r sum_syn, echo=FALSE}
summary(synth_bisaillon)
```

## Heat maps (correlation matrices)

These plots show the correlation between two variables. This type of data validation is useful if the data is Gaussian (i.e., it is normally distributed). If the data were skewed, this would not be a sufficient test as this only looks at the correlation between two variables at a time. This is an appropriate test if the intended use is to be able to perform calculations based on mean and/or covariance in one-dimension. Otherwise, if the intended use of the synthetic data is to look at relationships between multiple variables (e.g., sex, sat_v, and fy_gpa) then this test would not return that information appropriately. 

These heat maps can be used to preliminary verify how similar the synthetic data is to the original data, assuming that the data are normally distributed, by comparing the correlations of two variables in the respective data sets. The heat map makes it easy to visually compare if the two data sets are producing similar results.

To examine relationships between multiple variables at a time, we can look at the Earth Mover's Distance (Wasserstein) which will be discussed in the next section.

:::::::::::::: {.columns}
::: {.column width="50%"}

### Original data
```{r plot1, echo=FALSE, warning=FALSE}
# Compute the variance of x and the covariance or correlation of x and y if these are vectors
cormat <- round(cor(satgpa), 2)
upper_tri <- get_upper_tri(cormat)
# Convert an object into a molten data frame
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap
heat <-
  ggplot(data = melted_cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Pearson Correlation"
  ) +
  theme_minimal() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
    axis.text.x = element_text(
      vjust = 1,
      size = 12,
      hjust = 1
    ),
    axis.text.y = element_text(
      vjust = 1,
      size = 12,
      hjust = 1
    ),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.55, 0.65),
    legend.direction = "horizontal") +
  guides(fill = guide_colorbar(
    barwidth = 7,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  )) +
  coord_fixed()

print(heat)
```

:::
::: {.column width="50%"}

### Synthetic data
```{r plot2, echo=FALSE, warning=FALSE}
# Compute the variance of x and the covariance or correlation of x and y if these are vectors
cormat <- round(cor(synth_bisaillon), 2)
upper_tri <- get_upper_tri(cormat)
# Convert an object into a molten data frame
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap
heat <-
  ggplot(data = melted_cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Pearson Correlation"
  ) +
  theme_minimal() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
    axis.text.x = element_text(
      vjust = 1,
      size = 12,
      hjust = 1
    ),
    axis.text.y = element_text(
      vjust = 1,
      size = 12,
      hjust = 1
    ),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.55, 0.65),
    legend.direction = "horizontal") +
  guides(fill = guide_colorbar(
    barwidth = 7,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  )) +
  coord_fixed()

print(heat)
```
:::
::::::::::::::

> **Note:** `synthpop` generates randomized numbers for the synthetic data; therefore, multiple outputs may not produce the same numbers. However, the synthetic data outputs should still yield similar results so that we would only really need to show one comparison to demonstrate that the synthetic data shows similar correlations to the original data.

## Earth Mover's Distance

More information re: Earth Mover's Distance can be found [here](https://www.rdocumentation.org/packages/emdist/versions/0.3-1/topics/emd).

------

### Excerpts from the [emdist documentation](https://www.rdocumentation.org/packages/emdist/versions/0.3-1/topics/emd) 

#### Value

Earth Mover's Distance (EMD) between of the distributions `A` and `B`. If `A` and `B` are not distributions then `A` is the source and `B` is the target.

#### Details

`emd2d` interprets the two matrices `A` and `B` as a distribution over a two-dimensional grid. The distance between the grid points in each direction is defined by `xdist` and `ydist.` Both matrices must have the same dimensionality.

`emd` uses first column of each matrix as the weighs and the remaining columns as location coordinates in a up to four-dimensional space. `A` and `B` **must have the same number of columns**. `emdw` separates the weights from the location matrices but is otherwise identical to `emd`. `emdr` uses the original EMD implementation by Yossi Rubner from Stanford.

If case the two matrices `A` and `B` are not densities, the weighted sum of flows is normalized by the smaller total mass of the two. The version of the `emd` package released on CRAN contains only this implementation and all other functions are just front-ends for the call to `emdr.`

#### Note re: the distance argument

Distance (dist) to be used for the computation of the cost over the locations. Must be either `"euclidean"`, `"manhattan"` or a [metric over a] closure taking two vectors and returning a scalar number. The latter case is much less efficient because it requires R evaluation for every possible combination of flows.

------

The main thing to note about the difference between `emd` and `emd2d` is that `emd` looks at vectors whereas `emd2d` looks at points; therefore, the latter calculates more combinations than the former. This would also explain why the `emd2d` result shows a value that is "more distant" than the `emd` result. Essentially, because `emd2d` looks at more combinations it has more to iterate over. 

```{r emd, echo=FALSE, warning=FALSE}
# mat_satgpa <- satgpa %>% as.matrix()
# mat_synth <- synth_bisaillon %>% as.matrix()

# emd(mat_satgpa, mat_synth, dist = "euclidean")
# [1] 1.243486
# Time difference of 5.467048 secs

# emd2d(mat_satgpa, mat_synth, dist = "euclidean")
# [1] 2.925377
# Time difference of 30.70497 mins (for euclidean)
# Time difference of 33.62831 mins (for manhattan)
```

Below are the results and run-time for each EMD function:

Function | Result | Run-time
|---|---|---|
`emd` | 1.243486 | 5.467048 secs
`emd2d` | 2.925377 | 30.70497 mins

The result that is returned shows us how close our synthetic data is to the original data. Ideally, you would want a value of 0 (or close to 0). However, since we do not have a pre-determined threshold to gauge proximity it cannot be determined whether values returned are "close enough" to the original data set.

This threshold may differ for users depending on their use case. Nonetheless, this test can show how similar the data are to one another.

Which function should you use? It depends on your use-case.

If the goal is to create synthetic data that is as close as possible to the original data then it might be useful to use `emd2d` to get the most accurate picture (of the two functions). Keep in mind that this function takes significantly longer to run than `emd`. Regardless of which option you choose to run as your final test, it would be prudent to run `emd` first and see that result prior to running `emd2d`. The reason would be that `emd` is time-effective and if you return a high value with this function, you would probably return an even higher value with `emd2d`. In that, if you returned a value above your desired threshold, it might be more valuable to modify the parameters of your synthetic data before testing it further.

```{r, include=FALSE, warning=FALSE}

# Source R script from GitHub
satgpa <-
  read.csv("./data/satgpa.csv") %>%
  select(-sat_sum) 
  # removed this from the data set because it's dependent on sat_m and sat_v

synth_bisaillon <-
  read.csv("./synthpop/synth_data.csv")
```

# Record-matching as a privacy measure

Trying to match original records in synthetic data is a simple measure of privacy but a practical one. If a synthetic dataset were to be published, people would likely check whether they can find themselves in there.

## Full matches

### Can someone find themselves in the data?

If a record is fully replicated in the synthetic data, it is effectively disclosed.

Thankfully finding matches is easy to do in R. An inner-join of the original and synthetic data will quickly return all rows that match 100%.

```{r}
# Number of 100% matched rows
full_matches = merge(satgpa, synth_bisaillon) %>% nrow()

full_matches
```

```{r, echo=FALSE}
total_records = nrow(satgpa)
full_matches = merge(satgpa, synth_bisaillon) %>% nrow()

matches_df <- data.frame(
  label = c("Fully matched", "Not fully-matched"),
  value = c(full_matches, total_records - full_matches))

fig <- plot_ly(matches_df, labels = ~label, values = ~value, type = 'pie',
               textposition = 'inside',
               textinfo = 'label+percent',
               insidetextfont = list(color = '#FFFFFF'),
               hoverinfo = 'text',
               marker = list(colors = colors,
                             line = list(color = '#FFFFFF', width = 1)),
               showlegend = FALSE,
               width = 300, height = 300)
fig <- fig %>% layout(title = "How many records are fully matched",
                      xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
                      yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))

fig
```

## Partial matches

### Could someone recognize parts of their data?

Even if a record is not fully replicated, pieces of it may still be recognizable. For example, someone's neighbor might be able to make guesses with part of their data. Or if only 3 variables of a record are recognizable, it may still be perceived as disclosed.

In R it is possible to run the `merge()` function iteratively on all combinations of columns. The `combn()` function will return all permutations of a vectors.

```{r}
# Return all possible samples of size 2 from (a, b, c)
combn(c("a", "b", "c"), 2, simplify = FALSE)
```

Partial matches are more likely than full matches. The SAT dataset has a variable for sex, so it is nearly guaranteed that at least one column can be matched. As we try to match more and more columns, we get less matches.

```{r, echo=FALSE}
# Point to dataframes
original <- satgpa
syn_df <- synth_bisaillon

# Setup
num_cols <- length(original)
columns_names <- names(original)

base_df <- original
base_df$index <- seq_len(nrow(base_df))

syn_df <- synth_bisaillon
syn_df$syn_index <- seq_len(nrow(syn_df))

levels_at_match <- data.frame(
  index=integer(),
  syn_index=integer(),
  match_level=integer()
)

for (x in rev(seq_len(num_cols))) {
  # Find all combinations of columns of x length
  combos <- combn(columns_names, x, simplify = FALSE)
  # For each combination, find all inner joins
  matches <- lapply(combos,
                    function(combo) {
                      m <- merge(base_df, syn_df, by = combo)
                      if (nrow(m) == 0) return(NULL)
                      m <- m[, c("index", "syn_index")]
                      m$match_level <- x
                      m
                    }
  )
  # Combine the successful inner joins
  matches <- unique(do.call(rbind, matches))
  # Append them to the tracking dataset
  levels_at_match <- rbind(levels_at_match, matches)
  # Remove the indexes that were matched, to make future inner joins faster
  base_df <- base_df[!base_df$index %in% matches$index, ]
}

# Prepare data for graphing
uniques <- unique(levels_at_match[, c("index", "match_level")])
cummulative_counts <- list()
for (x in rev(seq_len(num_cols))) {
  cummulative_counts[x] <- nrow(unique(uniques[uniques$match_level >= x, ]))
}

cummulative_counts <- data.frame(match_level = seq_len(num_cols),
                                 n = unlist(cummulative_counts))
# Rebase as percentages
cummulative_counts$n <- 100 * cummulative_counts$n / nrow(original)
cummulative_counts$perc <- paste0(cummulative_counts$n, "%")

fig <- plot_ly(
  x = cummulative_counts$match_level,
  y = cummulative_counts$n,
  text = cummulative_counts$perc,
  hoverinfo = 'text',
  type = "bar",
  marker = list(line = list(color = 'rgb(8,48,107)',
                            width = 1.5)),
  width = 300, height = 300) %>%
  layout(title = "Partial matches of original data (SAT)",
         xaxis = list(title = "Columns to match"),
         yaxis = list(title = "Percentage of rows matched"),
         bargap = 0)

fig
```

## Grid search

Below are some very quick grid search results. To get more trustworthy results, we would have to run longer searches. These searches were done quickly in only a few hours.

### Variable sequences

In `./gridsearch/gridsearch.R` there are simple examples of an iterative grid search for `rsynthpop` synthesis.

A grid search is exhaustive trial of "hyper-parameters", which are settings set by the user. If the user doesn't know what settings are the best, a grid search will iterate over all of their combinations and return the best scores.

You can set the order that variables are synthesize in `rsynthpop`. It's clear what the best order is, so we can try all of the combinations. Since synthesis is random, it's a good idea to run tests multiple times and average them.

Below are the lowest earthmover scores for the sequences (averages of 3 trials each). `sat_sum` seems to be a good variable to start with.

```{r, echo=FALSE}
col_names <- c("sex", "hs_gpa", "fy_gpa", "sat_sum", "sat_v", "sat_m")

read.csv("./gridsearch/visit_sequences.csv") %>%
  mutate_at(c("s1", "s2", "s3", "s4", "s5", "s6"), function(x) col_names[x]) %>%
  group_by(s1, s2, s3, s4, s5, s6, method) %>%
  summarise(mean_em = mean(em), mean_matches = mean(matches)) %>%
  ungroup() %>%
  arrange(mean_em) %>%
  head()
```

### Methods

`rsynthpop` offers many different methods of synthesis. We can perform a grid search on these.

The following worked with the dataset: ctree, cart, rf, ranger, norm, normrank, sample

These didn't work, either because they're for categorical data or the dataset needed extra processing: bagging, survctree, lognorm, sqrtnorm, cubertnorm, logreg, polyreg, polr, pmm, passive, nested, satcat

Using the `sat_sum	sat_v	hs_gpa	sat_m	sex	fy_gpa` sequence (each run 10 times), we see that the random forest methods give the best scores. However, these earthmover scores are higher than the search 

```{r, echo=FALSE}
read.csv("./gridsearch/methods.csv") %>%
  group_by(method) %>%
  summarise(mean_em = mean(em), mean_matches = mean(matches)) %>%
  ungroup() %>%
  arrange(mean_em) %>%
  head()
```

## Python analysis

```{python}
import pandas as pd
import seaborn as sns
from scipy.stats import wasserstein_distance
import matplotlib.pyplot as plt
sns.set_theme(style="ticks")
```

```{python}
#Load CSV file in a dataframe
df = pd.read_csv('./data/satgpa.csv')
df = df.drop(columns = ['sat_sum'])
dfsynth = pd.read_csv('./synthpop/synth_data.csv')
```

### Visual inspection

```{python}
sns.pairplot(df, hue = "sex")
# Necessary to render in RStudio
# plt.show()
```

```{python}
sns.pairplot(dfsynth, hue = "sex")
# Necessary to render in RStudio
# plt.show()
```

### Comparing distributions

#### Wassertein

```{python}
print(f"Distance between sex is {wasserstein_distance(df['sex'].tolist(), dfsynth['sex'].tolist()):.3f}")
print(f"Distance between sat_v is {wasserstein_distance(df['sat_v'].tolist(), dfsynth['sat_v'].tolist()):.3f}")
print(f"Distance between sat_m is {wasserstein_distance(df['sat_m'].tolist(), dfsynth['sat_m'].tolist()):.3f}")
#print(f"Distance between sat_sum is {wasserstein_distance(df['sat_sum'].tolist(), dfsynth['sat_sum'].tolist()):.3f}")
print(f"Distance between hs_gpa is {wasserstein_distance(df['hs_gpa'].tolist(), dfsynth['hs_gpa'].tolist()):.3f}")
print(f"Distance between fy_gpa is {wasserstein_distance(df['fy_gpa'].tolist(), dfsynth['fy_gpa'].tolist()):.3f}")
```

#### Conditonal distributions

With `sex = 1`

```{python}
dfc = df[df['sex'] == 1]
dfsynthc = dfsynth[dfsynth['sex'] == 1]

print(f"Distance between sex is {wasserstein_distance(dfc['sex'].tolist(), dfsynthc['sex'].tolist()):.3f}")
print(f"Distance between sat_v is {wasserstein_distance(dfc['sat_v'].tolist(), dfsynthc['sat_v'].tolist()):.3f}")
print(f"Distance between sat_m is {wasserstein_distance(dfc['sat_m'].tolist(), dfsynthc['sat_m'].tolist()):.3f}")
#print(f"Distance between sat_sum is {wasserstein_distance(dfc['sat_sum'].tolist(), dfsynthc['sat_sum'].tolist()):.3f}")
print(f"Distance between hs_gpa is {wasserstein_distance(dfc['hs_gpa'].tolist(), dfsynthc['hs_gpa'].tolist()):.3f}")
print(f"Distance between fy_gpa is {wasserstein_distance(dfc['fy_gpa'].tolist(), dfsynthc['fy_gpa'].tolist()):.3f}")
```

With `sex = 2`

```{python}
dfc = df[df['sex'] == 2]
dfsynthc = dfsynth[dfsynth['sex'] == 2]

print(f"Distance between sex is {wasserstein_distance(dfc['sex'].tolist(), dfsynthc['sex'].tolist()):.3f}")
print(f"Distance between sat_v is {wasserstein_distance(dfc['sat_v'].tolist(), dfsynthc['sat_v'].tolist()):.3f}")
print(f"Distance between sat_m is {wasserstein_distance(dfc['sat_m'].tolist(), dfsynthc['sat_m'].tolist()):.3f}")
#print(f"Distance between sat_sum is {wasserstein_distance(dfc['sat_sum'].tolist(), dfsynthc['sat_sum'].tolist()):.3f}")
print(f"Distance between hs_gpa is {wasserstein_distance(dfc['hs_gpa'].tolist(), dfsynthc['hs_gpa'].tolist()):.3f}")
print(f"Distance between fy_gpa is {wasserstein_distance(dfc['fy_gpa'].tolist(), dfsynthc['fy_gpa'].tolist()):.3f}")
```